{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "049ded7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a8b4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Using cached chromadb-0.3.26-py3-none-any.whl (123 kB)\n",
      "Requirement already satisfied: pandas>=1.3 in ./lib/python3.9/site-packages (from chromadb) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.28 in ./lib/python3.9/site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in ./lib/python3.9/site-packages (from chromadb) (1.10.9)\n",
      "Collecting hnswlib>=0.7 (from chromadb)\n",
      "  Using cached hnswlib-0.7.0.tar.gz (33 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting clickhouse-connect>=0.5.7 (from chromadb)\n",
      "  Using cached clickhouse_connect-0.6.3-cp39-cp39-macosx_10_9_x86_64.whl (241 kB)\n",
      "Collecting duckdb>=0.7.1 (from chromadb)\n",
      "  Using cached duckdb-0.8.1-cp39-cp39-macosx_10_9_x86_64.whl (14.7 MB)\n",
      "Collecting fastapi>=0.85.1 (from chromadb)\n",
      "  Using cached fastapi-0.98.0-py3-none-any.whl (56 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
      "  Using cached uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: numpy>=1.21.6 in ./lib/python3.9/site-packages (from chromadb) (1.23.5)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Using cached posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./lib/python3.9/site-packages (from chromadb) (4.6.3)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
      "  Using cached pulsar_client-3.2.0-cp39-cp39-macosx_10_15_universal2.whl (10.8 MB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Using cached onnxruntime-1.15.1-cp39-cp39-macosx_10_15_x86_64.whl (6.8 MB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./lib/python3.9/site-packages (from chromadb) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./lib/python3.9/site-packages (from chromadb) (4.65.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./lib/python3.9/site-packages (from chromadb) (7.3.1)\n",
      "Requirement already satisfied: certifi in ./lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.5.7)\n",
      "Collecting urllib3>=1.26 (from clickhouse-connect>=0.5.7->chromadb)\n",
      "  Using cached urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "Requirement already satisfied: pytz in ./lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.3)\n",
      "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb)\n",
      "  Using cached zstandard-0.21.0-cp39-cp39-macosx_10_9_x86_64.whl (473 kB)\n",
      "Collecting lz4 (from clickhouse-connect>=0.5.7->chromadb)\n",
      "  Using cached lz4-4.3.2-cp39-cp39-macosx_10_9_x86_64.whl (254 kB)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.85.1->chromadb)\n",
      "  Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: packaging in ./lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
      "Requirement already satisfied: protobuf in ./lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (4.23.3)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./lib/python3.9/site-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.9/site-packages (from requests>=2.28->chromadb) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.9/site-packages (from requests>=2.28->chromadb) (3.4)\n",
      "Requirement already satisfied: click>=7.0 in ./lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\n",
      "Requirement already satisfied: h11>=0.8 in ./lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached httptools-0.5.0-cp39-cp39-macosx_10_9_x86_64.whl (161 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached uvloop-0.17.0-cp39-cp39-macosx_10_9_x86_64.whl (1.5 MB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached watchfiles-0.19.0-cp37-abi3-macosx_10_7_x86_64.whl (405 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached websockets-11.0.3-cp39-cp39-macosx_10_9_x86_64.whl (120 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in ./lib/python3.9/site-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (3.7.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in ./lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (1.1.1)\n",
      "Building wheels for collected packages: hnswlib\n",
      "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for hnswlib \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[17 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m creating var\n",
      "  \u001b[31m   \u001b[0m creating var/folders\n",
      "  \u001b[31m   \u001b[0m creating var/folders/85\n",
      "  \u001b[31m   \u001b[0m creating var/folders/85/pq1_g7yj2p51qn8fx6_cdts90ptw59\n",
      "  \u001b[31m   \u001b[0m creating var/folders/85/pq1_g7yj2p51qn8fx6_cdts90ptw59/T\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/Headers -arch arm64 -arch x86_64 -Werror=implicit-function-declaration -I/usr/local/opt/openssl@3/include -I/Users/princekumar.k/Documents/Mylearnings/GenAI/venv/env/include -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/include/python3.9 -c /var/folders/85/pq1_g7yj2p51qn8fx6_cdts90ptw59/T/tmptcgh9w8u.cpp -o var/folders/85/pq1_g7yj2p51qn8fx6_cdts90ptw59/T/tmptcgh9w8u.o -std=c++14\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/Headers -arch arm64 -arch x86_64 -Werror=implicit-function-declaration -I/usr/local/opt/openssl@3/include -I/Users/princekumar.k/Documents/Mylearnings/GenAI/venv/env/include -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/include/python3.9 -c /var/folders/85/pq1_g7yj2p51qn8fx6_cdts90ptw59/T/tmpyi3k0whn.cpp -o var/folders/85/pq1_g7yj2p51qn8fx6_cdts90ptw59/T/tmpyi3k0whn.o -fvisibility=hidden\n",
      "  \u001b[31m   \u001b[0m building 'hnswlib' extension\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-10.9-universal2-cpython-39\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-10.9-universal2-cpython-39/python_bindings\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/Headers -arch arm64 -arch x86_64 -Werror=implicit-function-declaration -I/usr/local/opt/openssl@3/include -I/private/var/folders/85/pq1_g7yj2p51qn8fx6_cdts90ptw59/T/pip-build-env-w7l858xw/overlay/lib/python3.9/site-packages/pybind11/include -I/private/var/folders/85/pq1_g7yj2p51qn8fx6_cdts90ptw59/T/pip-build-env-w7l858xw/overlay/lib/python3.9/site-packages/numpy/core/include -I./hnswlib/ -I/Users/princekumar.k/Documents/Mylearnings/GenAI/venv/env/include -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/include/python3.9 -c ./python_bindings/bindings.cpp -o build/temp.macosx-10.9-universal2-cpython-39/./python_bindings/bindings.o -O3 -march=native -stdlib=libc++ -mmacosx-version-min=10.7 -DVERSION_INFO=\\\"0.7.0\\\" -std=c++14 -fvisibility=hidden\n",
      "  \u001b[31m   \u001b[0m clang: error: the clang compiler does not support '-march=native'\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/clang' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for hnswlib\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build hnswlib\n",
      "\u001b[31mERROR: Could not build wheels for hnswlib, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f5edfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycairo\n",
      "  Using cached pycairo-1.24.0.tar.gz (344 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pycairo\n",
      "  Building wheel for pycairo (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pycairo \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[12 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-universal2-cpython-39\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-universal2-cpython-39/cairo\n",
      "  \u001b[31m   \u001b[0m copying cairo/__init__.py -> build/lib.macosx-10.9-universal2-cpython-39/cairo\n",
      "  \u001b[31m   \u001b[0m copying cairo/__init__.pyi -> build/lib.macosx-10.9-universal2-cpython-39/cairo\n",
      "  \u001b[31m   \u001b[0m copying cairo/py.typed -> build/lib.macosx-10.9-universal2-cpython-39/cairo\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m 'pkg-config' not found.\n",
      "  \u001b[31m   \u001b[0m Command ['pkg-config', '--print-errors', '--exists', 'cairo >= 1.15.10']\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for pycairo\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build pycairo\n",
      "\u001b[31mERROR: Could not build wheels for pycairo, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mzsh:1: command not found: apt-get\n",
      "zsh:1: command not found: apt-get\n",
      "Collecting manimlib\n",
      "  Using cached manimlib-0.2.0-py3-none-any.whl\n",
      "Collecting argparse (from manimlib)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting colour (from manimlib)\n",
      "  Using cached colour-0.1.5-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numpy in ./lib/python3.9/site-packages (from manimlib) (1.23.5)\n",
      "Requirement already satisfied: Pillow in ./lib/python3.9/site-packages (from manimlib) (9.5.0)\n",
      "Collecting progressbar (from manimlib)\n",
      "  Using cached progressbar-2.5-py3-none-any.whl\n",
      "Collecting scipy (from manimlib)\n",
      "  Using cached scipy-1.10.1-cp39-cp39-macosx_10_9_x86_64.whl (35.2 MB)\n",
      "Requirement already satisfied: tqdm in ./lib/python3.9/site-packages (from manimlib) (4.65.0)\n",
      "Collecting opencv-python (from manimlib)\n",
      "  Using cached opencv_python-4.7.0.72-cp37-abi3-macosx_10_16_x86_64.whl (53.9 MB)\n",
      "Collecting pycairo (from manimlib)\n",
      "  Using cached pycairo-1.24.0.tar.gz (344 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pydub (from manimlib)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: pygments in ./lib/python3.9/site-packages (from manimlib) (2.15.1)\n",
      "Building wheels for collected packages: pycairo\n",
      "  Building wheel for pycairo (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pycairo \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[12 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-universal2-cpython-39\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-universal2-cpython-39/cairo\n",
      "  \u001b[31m   \u001b[0m copying cairo/__init__.py -> build/lib.macosx-10.9-universal2-cpython-39/cairo\n",
      "  \u001b[31m   \u001b[0m copying cairo/__init__.pyi -> build/lib.macosx-10.9-universal2-cpython-39/cairo\n",
      "  \u001b[31m   \u001b[0m copying cairo/py.typed -> build/lib.macosx-10.9-universal2-cpython-39/cairo\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m 'pkg-config' not found.\n",
      "  \u001b[31m   \u001b[0m Command ['pkg-config', '--print-errors', '--exists', 'cairo >= 1.15.10']\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for pycairo\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build pycairo\n",
      "\u001b[31mERROR: Could not build wheels for pycairo, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip3 install p5py\n",
    "# !pip3 install PEP517\n",
    "!pip3 install pycairo\n",
    "!apt-get install sox ffmpeg libcairo2 libcairo2-dev\n",
    "!apt-get install texlive-full\n",
    "!pip3 install manimlib  # or pip install manimlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1d25f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import PyPDFLoader\n",
    "# from langchain.document_loaders import CSVLoader\n",
    "# file = 'OutdoorClothingCatalog_1000.csv'\n",
    "# loader1 = CSVLoader(file_path=file)\n",
    "# pdfFile = '/OfferCreationviaGrumbles.pdf'\n",
    "\n",
    "# loader1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e2f135c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOP/Incentivised Attach SOP.pdf\n",
      "SOP/Targeted Discounts - SOP.pdf\n",
      "SOP/Bump Up Offers.pdf\n",
      "SOP/Casper SOP.pdf\n",
      "SOP/Targeted Discount on FP - SOP.pdf\n",
      "SOP/_SOP for Shipping Rule Creation.pdf\n",
      "SOP/Exchange Offers.pdf\n",
      "SOP/Prebook Offer.pdf\n",
      "SOP/Shipping Offers - SOP.pdf\n",
      "SOP/Either Or offers SOP.pdf\n",
      "SOP/Promise Callout Offers - SOP.pdf\n",
      "SOP/BMGN v1 Offer SOP.pdf\n",
      "SOP/Callout Offer.pdf\n",
      "SOP/CnC Offers - SOP.pdf\n",
      "SOP/Multi Unit Listing Offers - SOP.pdf\n",
      "SOP/Seller Recommendation for an Offer.pdf\n",
      "SOP/Offer Creation via Grumbles .pdf\n",
      "SOP/Basket Offers - SOP (1).pdf\n",
      "SOP/Early Access SOP.pdf\n",
      "SOP/Combo Offers - SOP.pdf\n",
      "SOP/PBO SOP.pdf\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "desktop = pathlib.Path(\"./SOP\")\n",
    "files = []\n",
    "list(desktop.iterdir())\n",
    "pages1 = []\n",
    "# for item in list(desktop.iterdir())[:1]:\n",
    "for item in list(desktop.iterdir()):\n",
    "    try:\n",
    "        print(str(item))\n",
    "        fileLoader = PyPDFLoader(str(item))\n",
    "#         pages = fileLoader.load_and_split()\n",
    "#         print(len(fileLoader.load_and_split()))\n",
    "        pages1.append(fileLoader.load_and_split())\n",
    "#         files.append(pages)\n",
    "    except:\n",
    "        print(\"An exception occurred\")\n",
    "  \n",
    "print (type(pages1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfd78bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "207\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pages = []\n",
    "print(len(pages1))\n",
    "for item in pages1:\n",
    "    try:\n",
    "        if(item.page_content):\n",
    "            pages.append(item)\n",
    "            print(item)\n",
    "    except:\n",
    "        for item2 in item:\n",
    "            pages.append(item2)\n",
    "print(len(pages))\n",
    "\n",
    "# context = \"\\n\".join(str(p.page_content) for p in pages[:7])\n",
    "# print(type(context))\n",
    "# print(len(context))\n",
    "# # print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebd6ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_llm_text = VertexAI(model_name=\"text-bison@001\")\n",
    "vertex_embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d85ea93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=10000, chunk_overlap=0)\n",
    "context = \"\\n\\n\".join(str(p.page_content) for p in pages)\n",
    "texts = text_splitter.split_text(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e1cf517",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not import chromadb python package. Please install it with `pip install chromadb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Mylearnings/GenAI/venv/env/lib/python3.9/site-packages/langchain/vectorstores/chroma.py:69\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import Chromadb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# import chromadb.config\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m vector_index \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertex_embeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mas_retriever()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# import chromadb\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# # setup Chroma in-memory, for easy prototyping. Can add persistence easily!\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# client = chromadb.Client()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     # where_document={\"$contains\":\"search_string\"}  # optional filter\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Mylearnings/GenAI/venv/env/lib/python3.9/site-packages/langchain/vectorstores/chroma.py:394\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    376\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[1;32m    377\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[0;32m~/Documents/Mylearnings/GenAI/venv/env/lib/python3.9/site-packages/langchain/vectorstores/chroma.py:72\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import chromadb python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install chromadb`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m client\n",
      "\u001b[0;31mValueError\u001b[0m: Could not import chromadb python package. Please install it with `pip install chromadb`."
     ]
    }
   ],
   "source": [
    "# import Chromadb\n",
    "# import chromadb.config\n",
    "vector_index = Chroma.from_texts(texts, vertex_embeddings).as_retriever()\n",
    "\n",
    "# import chromadb\n",
    "# # setup Chroma in-memory, for easy prototyping. Can add persistence easily!\n",
    "# client = chromadb.Client()\n",
    "\n",
    "# # Create collection. get_collection, get_or_create_collection, delete_collection also available!\n",
    "# collection = client.create_collection(\"all-my-documents\")\n",
    "\n",
    "# # Add docs to the collection. Can also update and delete. Row-based API coming soon!\n",
    "# collection.add(\n",
    "#     documents=[\"This is document1\", \"This is document2\"], # we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well\n",
    "#     metadatas=[{\"source\": \"notion\"}, {\"source\": \"google-docs\"}], # filter on these!\n",
    "#     ids=[\"doc1\", \"doc2\"], # unique for each doc\n",
    "# )\n",
    "\n",
    "# # Query/search 2 most similar results. You can also .get by id\n",
    "# results = collection.query(\n",
    "#     query_texts=[\"This is a query document\"],\n",
    "#     n_results=2,\n",
    "#     # where={\"metadata_field\": \"is_equal_to_this\"}, # optional filter\n",
    "#     # where_document={\"$contains\":\"search_string\"}  # optional filter\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea2b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_answer = stuff_chain(\n",
    "    {\"input_documents\": pages[:7], \"question\": question}, \n",
    "    return_only_outputs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc868b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stuff_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25031e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566fb2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mp_data = []\n",
    "\n",
    "# for each document, extract metadata and intermediate steps of the MapReduce process\n",
    "for doc, out in zip(\n",
    "    map_reduce_outputs[\"input_documents\"], map_reduce_outputs[\"intermediate_steps\"]\n",
    "):\n",
    "    output = {}\n",
    "    output[\"file_name\"] = p(doc.metadata[\"source\"]).stem\n",
    "    output[\"file_type\"] = p(doc.metadata[\"source\"]).suffix\n",
    "    output[\"page_number\"] = doc.metadata[\"page\"]\n",
    "    output[\"chunks\"] = doc.page_content\n",
    "    output[\"answer\"] = out\n",
    "    final_mp_data.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d1692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe from a dictionary\n",
    "pdf_mp_answers = pd.DataFrame.from_dict(final_mp_data)\n",
    "# sorting the dataframe by filename and page_number\n",
    "pdf_mp_answers = pdf_mp_answers.sort_values(by=[\"file_name\", \"page_number\"])\n",
    "pdf_mp_answers.reset_index(inplace=True, drop=True)\n",
    "pdf_mp_answers.head()\n",
    "print(pdf_mp_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7092820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install docarray\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    pages, \n",
    "    vertex_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b345fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b13aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### we will use above vectorstore to find answer to query\n",
    "# query = \"What is Incentivised offer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## find all similar document\n",
    "# docs = db.similarity_search(query)\n",
    "# print(len(docs))\n",
    "## 4 docs are returned\n",
    "# docs[0] ## indeed a shirt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9349f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f30c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "\n",
    "def getResponse(prompt ,temperature: float = 0.2):\n",
    "    \"\"\"Ideation example with a Large Language Model\"\"\"\n",
    "\n",
    "    # TODO developer - override these parameters as needed:\n",
    "    parameters = {\n",
    "        \"temperature\": temperature,  # Temperature controls the degree of randomness in token selection.\n",
    "        \"max_output_tokens\": 256,  # Token limit determines the maximum amount of text output.\n",
    "        \"top_p\": 0.8,  # Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\n",
    "        \"top_k\": 40,  # A top_k of 1 means the selected token is the most probable among all tokens.\n",
    "    }\n",
    "\n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    response = model.predict(\n",
    "        prompt,\n",
    "        **parameters,\n",
    "    )\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from vertexai.preview.language_models import ChatModel, InputOutputTextPair\n",
    "\n",
    "# def getChatResponse(prompt, temperature: float = 0.2):\n",
    "#     chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
    "\n",
    "#     # TODO developer - override these parameters as needed:\n",
    "#     parameters = {\n",
    "#         \"temperature\": temperature,  # Temperature controls the degree of randomness in token selection.\n",
    "#         \"max_output_tokens\": 256,  # Token limit determines the maximum amount of text output.\n",
    "#         \"top_p\": 0.95,  # Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\n",
    "# #         \"top_k\": 40,  # A top_k of 1 means the selected token is the most probable among all tokens.\n",
    "#     }\n",
    "\n",
    "#     chat = chat_model.start_chat(\n",
    "#         context=SOPContext,\n",
    "#         examples=[\n",
    "#             InputOutputTextPair(\n",
    "#             ),\n",
    "#         ],\n",
    "#     )\n",
    "\n",
    "#     response = chat.send_message(\n",
    "#         query, **parameters\n",
    "#           )\n",
    "#     print(f\"Response from Model: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb258bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"how does deferred Giveaway works for offer\"\n",
    "# query = \"how does I create a Promise Callout Offer\"\n",
    "query = \"What is insufficient participation ?\"\n",
    "relevantDocs = db.max_marginal_relevance_search(query, k=20)\n",
    "# relevantDocs = db.similarity_search_with_score(query, k=10)\n",
    "\n",
    "print(len(relevantDocs))\n",
    "# print(relevantDocs)\n",
    "\n",
    "qdocs = \"\".join([relevantDocs[i].page_content for i in range(len(relevantDocs))])\n",
    "# print(qdocs)\n",
    "for i in range(0,len(qdocs)-2):\n",
    "    print(relevantDocs[i])\n",
    "    relevantDocs[i].page_content\n",
    "    print(item, \"======\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad6316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "role = \"\"\"\n",
    "You are offer related query answering module for promotions platform on Flipkart.\\\n",
    "You answer questions with a high degree of precision & wherever you do not have enough precision, \\\n",
    "you always provide an output saying the answers might not be fully correct. \\\n",
    "Promotions Platform has capabilities to create and maintain an offer via its \\\n",
    "offer creation console- 'Grumbles' and maintenance console-- 'Casper'. Business users\\\n",
    "can ask multiple questions related to capabilities for creating and maintaining an offer.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\" \n",
    "Role: {role}\n",
    "Follow exactly these 3 steps:\n",
    "1. Read the context below and aggregrate this data\n",
    "Context : {qdocs}\n",
    "2. Answer the question using only this context\n",
    "3. Show the source for your answers\n",
    "User Question: {query}\n",
    "\n",
    "If you don't have any context and are unsure of the answer, reply that you don't know about this topic.\n",
    "\n",
    "\"\"\"\n",
    "print(len(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff81a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getResponse(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4808cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# response = vertex_llm_text.call_as_llm(f\"{qdocs} Question: What is Incentivised offer.\") \n",
    "\n",
    "# print (sopContext)\n",
    "\n",
    "# print(prompt)\n",
    "# question =  f\"{qdocs} {prompt}\"\n",
    "# print(question)\n",
    "# print(getResponse(question))\n",
    "# display(Markdown(response))\n",
    "\n",
    "# getChatResponse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3893e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c3e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
